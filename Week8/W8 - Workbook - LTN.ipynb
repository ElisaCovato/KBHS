{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a31212a",
   "metadata": {},
   "source": [
    "# Neuro-symbolic AI - LTN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1070f",
   "metadata": {},
   "source": [
    "This workbook explores the concepts of **hybrid intelligent systems**, in particular of how to combine a knowledge-base structure with a neural network, in other words, how to create a **neuro-symbolic** solution. \n",
    "\n",
    "\n",
    "Throughout this notebook, you will work on simple hybrid systems, using logic and, of course, neural networks.You will find some guided examples to aid your understanding, and some exercises for you to implement on your own.\n",
    "\n",
    "#### Content:\n",
    "* [LTN - Logical Tensor Networks](#ltn)\n",
    "    * [Getting started](#ltn-start)\n",
    "    * [Exercise: Wine classification](#ltn-ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fc2ea",
   "metadata": {},
   "source": [
    "## LTN - Logic Tensor Network <a class=\"anchor\" id=\"ltn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acdc72",
   "metadata": {},
   "source": [
    "[Logic tensor networks (LTN)](https://arxiv.org/pdf/2012.13635.pdf) are a framework that combines neural networks with first-order logic to enable machines to reason and make decisions based on logical rules. Logical propositions are used to represent the KB as formulas, and the neural /deep learning part is used to learn the different weights of these formulas. These logical propositions integrate prior domain knowledge into the neural network, and act as constraints on the neural network’s performance: if the neural network’s output violates the logical propositions, then it is penalised. This means that during training, an LTN does not need only to improve its predictive power, but it has also satisfy the logical propositions.\n",
    "\n",
    "In what follows we will use [LTNtorch](https://github.com/tommasocarraro/LTNtorch), an LTN implementation based on the deep learning package [PyTorch](https://github.com/pytorch/pytorch) (similar to Tensorflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a261ced",
   "metadata": {},
   "source": [
    "### Getting started <a class=\"anchor\" id=\"gcn-start\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badde206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install LTNtorch, make sure to have also pytorch installed\n",
    "# !pip install LTNtorch\n",
    "\n",
    "import torch\n",
    "import ltn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed368a",
   "metadata": {},
   "source": [
    "### Exercise - wine classification <a class=\"anchor\" id=\"ltn-ex\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed54f0",
   "metadata": {},
   "source": [
    "The following exercise is adapted from _Dingli, A. and Farrugia, D. (2023) Neuro-Symbolic AI_ book.\n",
    "\n",
    "We are going to implement a simple LTN to classify type of wines. The dataset used is the [red & wine dataset](https://www.kaggle.com/datasets/numberswithkartik/red-white-wine-dataset). The dataset contains 6000 data points and 11 features describing different wine characteristics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5e8b9",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e2a5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "wine_df = pd.read_csv('./wine_dataset.csv')\n",
    "\n",
    "wine_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafc4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples per class\n",
    "wine_df['style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5e129",
   "metadata": {},
   "source": [
    "The dataset is fairly clean (alotugh imbalance!), and we need to only perform the following processing steps:\n",
    "- Convert to binary the label `style` column\n",
    "- Separate features from label\n",
    "- Normalise the features (right now all numerical features have different scales).\n",
    "- Create train/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert style to binary True=red / False=white\n",
    "wine_df['style'] = np.where(wine_df['style'] == 'red', True, False)\n",
    "\n",
    "# sparate features from label\n",
    "X = wine_df.loc[:, wine_df.columns != 'style'].values\n",
    "y = wine_df['style']\n",
    "\n",
    "# standardise the features\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# split dataset: train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a04d43",
   "metadata": {},
   "source": [
    "The features and labels created need to be converted into [Pytorch tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html),  multi-dimensional arrays that allow for efficient computation within the Pytorch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e44afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "X_train = torch.as_tensor(X_train).to(dtype=torch.float32)\n",
    "y_train = torch.as_tensor(y_train.values).to(dtype=torch.float32)\n",
    "X_test = torch.as_tensor(X_test).to(dtype=torch.float32)\n",
    "y_test = torch.as_tensor(y_test.values).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88be273",
   "metadata": {},
   "source": [
    "While training a model, we  want to pass data in 'batches', reshuffled data at every epoch to reduce model overfitting. In PyTorch this is done using [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders), an iterable pytorch construct that allows to create batches to feed into the model. \n",
    "\n",
    "The input to `DataLoader` is a `TensorDataset`, a combination of the features and label tensors into a unique dataset. \n",
    "\n",
    "DataLoader will randomly put data into batches, possibly creating batches with all 0s or 1s. We want to make sure tha each batch has some samples for each class. For this reason, we are going to create a `balanced_sampler` (see code below), to pass to DataLoader and create less random and more balanced batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# cont how many samples for each class are in the training set\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
    "\n",
    "# compute the inverse of the previous count\n",
    "# this will represent the weight for each class\n",
    "weight = 1. /class_sample_count\n",
    "\n",
    "# associate to each class the corresponding weight\n",
    "samples_weight = np.array([weight[t.int().item()] for t in y_train])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "\n",
    "# create the balanced sampler helper to pass to DataLoader\n",
    "balanced_sampler = WeightedRandomSampler(\n",
    "    samples_weight.type('torch.DoubleTensor'), \n",
    "    len(samples_weight))\n",
    "\n",
    "\n",
    "# create train and test Tensordataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# create train and test DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, sampler=balanced_sampler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc0104",
   "metadata": {},
   "source": [
    "**Neural Network**\n",
    "\n",
    "The neural network part of our LTN system will be a very simple **feed forward neural network _N_**, with the following characteristics:\n",
    "\n",
    "- **Input layer**: (11,64)\n",
    "- **Hidden layer**: (64, 64)\n",
    "- **Output layer**: (64,1)\n",
    "\n",
    "Let's define the model using PyTorch syntax, so we will be able to use it within the LTNTorch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b815aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # define model components\n",
    "        super(ModelN, self).__init__()\n",
    "        \n",
    "        # aactivation functions\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # layers\n",
    "        self.layer1 = torch.nn.Linear(11, 64)\n",
    "        self.layer2 = torch.nn.Linear(64, 64)\n",
    "        self.layer3 = torch.nn.Linear(64, 1)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "    # construct the feed-forward network\n",
    "    def forward(self, x):\n",
    "        # layer 1: activate input layer (11,64) using ReLu function\n",
    "        x = self.relu(self.layer1(x))\n",
    "        # layer 2 : activate hidden layer (64,64) using ReLu function\n",
    "        x = self.relu(self.layer2(x))\n",
    "        # add dropout with probability 0.1\n",
    "        x = self.dropout(x)\n",
    "        # layer 3: return output layer (64,1) with sigmoid function\n",
    "        return self.sigmoid(self.layer3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017f1475",
   "metadata": {},
   "source": [
    "The code above is equivalent to the following Tensorflow code\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(11,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae94626",
   "metadata": {},
   "source": [
    "**Logic KB: Predicates, Connectives and Quantifiers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac8e41",
   "metadata": {},
   "source": [
    "In a LTN, the knowledge-base is represented by first-order-logic (FOL) elements. \n",
    "\n",
    "On Week2, we have seen that FOL improves the classic logic (simple true/false statements) by providing **generalisations** with **predicates** and **quantifiers**. For example, a FOL element is:\n",
    "\n",
    "<div style='text-align: center;'>\n",
    "∀ x (Sunny(x) ∧ ￢ Weekend(x) -> ￢ Hiking(me)) \n",
    "</div>\n",
    "\n",
    "In plain English: for any days of the week, which is sunny but not the weekend, then I don't go hiking. In particular, the statement above has the following components:\n",
    "\n",
    "* **predicates**: Sunny(x), Weekend(x), are all predicates, where the true/false value depends on the specific value of the _variable_ x (=day of the week)\n",
    "* **connectives**: ∧,  ￢, ->, these are  connectives, allowing multiple predicates to be combined together\n",
    "* **quantifiers**: ∀ (for all) allows the full combination of predicates to be generalised to any day of the week.\n",
    "\n",
    "To create the KB for our neurosymbolic solution, we need to define the predicates, connectives and quantifiers related to our dataset.\n",
    "\n",
    "As seen above, the predicate is a 'function' that maps a variable x to a true/false state. For this reason, we can choose the _predicate_ to be the simple **feed forward neural network _N_** defined before: for each data point x in the wine dataset, it will predict (= map) this variable into a label of true(= red) or false(= white). We can therefore write:\n",
    "\n",
    "<div style='text-align: center;'>\n",
    "    <i>N</i>(x,L) \n",
    "</div>\n",
    "\n",
    "In plain English: the network _N_ will associate (predict) the data point x to the label L(red/white, true/flase). \n",
    "\n",
    "Another fact that we know about our dataset is that the two possible labels (red/white) are mutually exclusive: if a wine gets predcited as red cannot (and is not!) be predicted as white.\n",
    "\n",
    "Given all these facts we can define our **knowledge base** with the following logic rules:\n",
    "<div style='text-align: center; white-space: pre-wrap;'>\n",
    "    1. <i>N</i>(x<sub>r</sub>,L<sub>r</sub>)\n",
    "    2. <i>N</i>(x<sub>w</sub>,L<sub>w</sub>) \n",
    "    3. ∀ x<sub>L</sub>: <i>N</i>(x<sub>L</sub>, L)\n",
    "    4. ∀ x<sub>￢L</sub>: ￢<i>N</i>(x<sub>￢L</sub>, ￢L)\n",
    "</div>\n",
    "\n",
    "Let's try to undertsand these rules:\n",
    "\n",
    "* <strong><i>N</i>(x<sub>r</sub>,L<sub>r</sub>)</strong> and <strong><i>N</i>(x<sub>w</sub>,L<sub>w</sub>)</strong>: here x<sub>r</sub> and x<sub>w</sub> are data points in our training set for which we already know their classification/label, red and white, respectively. These rules are to ensure that the network correctly classify what is already known (hence true), in other words <i>N</i>(x<sub>r</sub>,L<sub>w</sub>)  or <i>N</i>(x<sub>w</sub>,L<sub>r</sub>) would be false (and a contradiction). The idea behind these rules is to ensure that the neural network _N_ reasons without contradictions in the **training phase**: all the true facts have to be considered as such throughout the whole training process.\n",
    "* <strong>∀ x<sub>L</sub>: <i>N</i>(x<sub>L</sub>, L)</strong>: here x<sub>L</sub> is any variable in our dataset, for which a label L is associated. This rule, together with <strong>∀ x<sub>￢L</sub>: ￢<i>N</i>(x<sub>￢L</sub>, ￢L)</strong>, is meant to ensure the _mutual exclusivity_ of our classification: if a data point x gets classified/predicted as L, it cannot be predcited simultaneosuly as the opposite, ￢L. In other words, these rules are based on the _laws of logic_ seen during Week 2: if something is True cannot be also False (law of contradiction). The idea behind these rules is to ensure that the neural network _N_ reasons without contradictions in the **prediction and optimisation phase**: if a contradiction is reached, the networks is failing. \n",
    "\n",
    "It's now time to code our KB. Given the rules above we will need a predciate, a connective ￢, and a quantifier ∀.  First we create the predicate _N_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312912e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predicate N (i.e., our Neural Network)\n",
    "N = ltn.Predicate(ModelN())\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852edb19",
   "metadata": {},
   "source": [
    "Create the NOT (￢) connective. \n",
    "\n",
    "In LTN,  a connective is created using the constructor `Connective()`, which takes as input a _fuzzy connective semantics_ from the `ltn.fuzzy_ops` module. As we have seen in Week6, fuzzy logic extend binary logic by considering all the values between 1=True and 0=False, so binary logic is just a special case of fuzzy logic. The reason why we use fuzzy semantic is to include all the values in the range [0,1]. \n",
    "\n",
    "If you are still wondering why we use fuzzy logic in a LTN, think about the output of a NN for binary classification: the sigmoid function squashes the output of the neural network to a value between 0 and 1, which can be interpreted as the probability of the input belonging to a certain class. This is the reason why we extend the binary logic to fuzzy logic in creating our connectives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the NOT standard connective\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "print(Not)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3cead",
   "metadata": {},
   "source": [
    "Finally create the FOR ALL (∀) quantifier.\n",
    "\n",
    "In LTN, a quantifier is created using the constructor `Quantifier()`, which takes as input an aggregation semantics and a character indicating which type of quantification is associated to the quantifier ('e' for exists, 'f' for forall). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the FOR ALL quantifier\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2),\n",
    "                        quantifier=\"f\")\n",
    "print(Forall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4db3b",
   "metadata": {},
   "source": [
    "_Error aggregation_ is a statistical technique that combines multiple error measures into a single value. In particular, **p-mean error aggregation** (`AggregPMeanError`)  calculates the average of the p-th power of individual error measures. In the case above, p=2. In a LTN, we use p-mean error aggregation to optimise the network performance since we want our KB rules to hold **for all** data points in the dataset, hence by aggregating errors we can assess the network ability to generalise to all data points.\n",
    "\n",
    "The connective and quantifier defined above, and their specific definition, are the recommended approach by LTNtorch for binary classification. For more complex and different situation, refer to the [LTNtorch example page](https://github.com/tommasocarraro/LTNtorch/tree/main/examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052c5e9",
   "metadata": {},
   "source": [
    "**Optmisation**\n",
    "\n",
    "We are almost there! The last thing to define is a way to optimise our solution. Since we are dealing with an hybrid system, we need to optimise two aspect of the system: the neural network prediction performance, and the neural network abilitty to reason without contradiction. in the first case we will use an Adam optimiser, as usually is done for neural networks. To optimise the reasoning we use `SatAgg` , the satisfaction aggregator, an operator which aggregates the truth values of _all_ the rules included in the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407dd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "optimizer = torch.optim.Adam(N.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e93e9b",
   "metadata": {},
   "source": [
    "**Training!**\n",
    "\n",
    "Finally, it's time to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835da472",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # reset the training loss for every epoch\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # start batching the data\n",
    "    for batch_idx, (data, labels) in enumerate(train_dataloader):\n",
    "        # transform the data points into logic variables\n",
    "        x_L = ltn.Variable(\"x_L\", data[torch.nonzero(labels)]) # positive examples\n",
    "        x_not_L = ltn.Variable(\"x_not_L\", data[torch.nonzero(torch.logical_not(labels))]) # negative examples\n",
    "\n",
    "        # compute SAT(isfaction) level\n",
    "        # the loss of our NN will depend on the ability of the network to satify the non-contradiction rules       \n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_L, N(x_L)),\n",
    "            Forall(x_not_L, Not(N(x_not_L))))\n",
    "\n",
    "        # compute loss and perform back-propagation\n",
    "        # set the gradients to zero before starting the backpropagation of the training process\n",
    "        # see this thread for more details: \n",
    "        # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch/48009142#48009142\n",
    "        optimizer.zero_grad()\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {train_loss / 2000:.3f}')\n",
    "            train_loss = 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e7b05",
   "metadata": {},
   "source": [
    "Let's now check the accuracy of our LTN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e91b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = 0.0\n",
    "# iterate over our data samples\n",
    "for data, labels in test_dataloader:\n",
    "    # get the predictions for the given samples\n",
    "    predictions = N.model(data).detach().numpy()\n",
    "\n",
    "    # convert the predictions to a binary classification (i.e., 0 or 1)\n",
    "    predictions = np.where(predictions > 0.5, 1., 0.).flatten()\n",
    "\n",
    "    # compute the accuracy_score\n",
    "    mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "# get the mean accuracy\n",
    "mean_accuracy / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc51ab",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "\n",
    "Try to run the same Neural Network but without using the logical KB (you can use tensorflow if you prefer). How does it compare to the LTN solution in terms of accuracy and training speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0832dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120d667",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "Implement a LTN for multi-label classification. \n",
    "\n",
    "* Option 1 (easy): Use the famous [Iris dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) to predic the 3 classes (0 = Setosa, 1 = Versicolour, 2 = Virginica) of iris flowers.\n",
    "\n",
    "* Option 2: Pick your favourite multi-label dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99136e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here your code\n",
    "\n",
    "\n",
    "# For option 1 uncomment the code below:\n",
    "# from sklearn import datasets\n",
    "# iris = datasets.load_iris()\n",
    "# features = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "# print(features.head())\n",
    "# label = pd.DataFrame(iris.target, columns=['label'])\n",
    "# print(label.sample(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
