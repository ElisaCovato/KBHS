{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a31212a",
   "metadata": {},
   "source": [
    "# Neuro-symbolic AI - GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1070f",
   "metadata": {},
   "source": [
    "This workbook explores the concepts of **hybrid intelligent systems**, in particular of how to combine a knowledge-base structure with a neural network, in other words, how to create a **neuro-symbolic** solution. \n",
    "\n",
    "\n",
    "Throughout this notebook, you will work on three simple hybrid systems, using graphs and, of course, neural networks. You will find some guided examples to aid your understanding, and some exercises for you to implement on your own.\n",
    "\n",
    "#### Content:\n",
    "* [GNN - Graph Neural Networks](#gnn)\n",
    "    * [Getting started](#gnn-start)\n",
    "    * [Exercise: node-level task](#gnn-node)\n",
    "    * [Exercise: edge-level task](#gnn-edge)\n",
    "    * [Exercise: graph-level task](#gnn-graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1167bed3",
   "metadata": {},
   "source": [
    "## GNN - Graph Neural Networks <a class=\"anchor\" id=\"gnn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d15094",
   "metadata": {},
   "source": [
    "**Graph Neural Networks (GNNs)** are a type of neural networks designed to work with **graph data**, structured representation of data as _nodes_ and _edges_ (see Week3 and Week4 learning material for more details). GNNs can be used in various applications like social network analysis, recommendation systems, and drug discovery. \n",
    "\n",
    "GNNs accept a graph as input, with information loaded into its nodes, edges and global-context. Through the layers of the neural network, GNNs capture the relationships and dependencies in the data  without changing the connectivity of the input graph.\n",
    "\n",
    "The picture below shows the structure of a GNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792f9ea",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"gnn_structure.png\" alt=\"GNN structure\" style=\"width: 800px;\"/>\n",
    "<figcaption style=\"text-align:center;font-style:italic\">(from <a href='https://tkipf.github.io/graph-convolutional-networks/'>Kipf, T.(2016) \"Graph Convolutional Neural Network\"</a> ) </figcaption>    \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2094020",
   "metadata": {},
   "source": [
    "In a GNN, the layers are responsible for aggregating information from neighboring nodes and updating a node features iteratively. In most GNN, this  process is performed through **message passing**. Message passing is a key concept in GNNs, mainly involving 3 key steps:\n",
    "\n",
    "1. **gathering**: for each node in the graph, the layer gathers all neighboring node features (also called _messages_) using a defined function.\n",
    "2. **aggregating**: aggregate all gathered messages using a function like sum to create a pooled message.\n",
    "3. **updating**: pass the pooled messages through an update function, to update the node information for the next layer.\n",
    "\n",
    "This process is shown in the picture below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e2283",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"message_passing.png\" alt=\"GNN message passing\" style=\"width: 800px;\"/>\n",
    "<figcaption style=\"text-align:center;font-style:italic\">(from <a href='https://distill.pub/2021/gnn-intro'>Sanchez-Lengeling, et al., \"A Gentle Introduction to Graph Neural Networks\", Distill, 2021</a> ) </figcaption>    \n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c1419",
   "metadata": {},
   "source": [
    "There are different type of GNNs. The two most important are Graph Convolutional Networks (GCNs) and Attention-based GNNs (GAT):\n",
    "\n",
    "* [**Graph Convolutional Networks (GCN)**](https://openreview.net/pdf?id=SJU4ayYgl): this specific type of GNNs  use **convolution operations** to update node representations during the message passing phase, similarly to the convolution operations used on Convolutional Neural Netowrks (CNN) for image processing. GCNs are effective for tasks where nodes have a clear notion of ordering and locality in the graph structure (for example in human-pose based tasks)\n",
    "* [**Graph Attention Networks (GAT)**](https://arxiv.org/pdf/1710.10903.pdf): in this case **attentions mechanism** are used to assign different weights to different neighbors based on their relevance, instead of using fixed weights for aggregating information during message passing. GATs are particularly useful when nodes have varying importance (for example for social network analysis tasks), or when capturing long-range dependencies in the graph.\n",
    "\n",
    "A detailed (but comprehensible) description of GNNs characteristics can be found here: https://distill.pub/2021/gnn-intro/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9ba60",
   "metadata": {},
   "source": [
    "GNNs are used for 3 tasks:\n",
    "* **node classification**: predicting the label of a node in a graph\n",
    "* **link prediction**: predicting the existence of edges between nodes\n",
    "* **graph classification**: classify and entire graph based on its structural properties\n",
    "\n",
    "Below we are going to look into these 3 tasks in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2666e8e",
   "metadata": {},
   "source": [
    "### Getting started <a class=\"anchor\" id=\"gnn-start\"></a>\n",
    "\n",
    "Let's start by importing all the libraries we need. We will implement our model using the deep learning package [PyTorch](https://github.com/pytorch/pytorch) (similar to Tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following if you do not have PyTorch already installed\n",
    "# !pip install torch\n",
    "\n",
    "# uncomment the following if you do not have PyTorch Geoemtric already installed\n",
    "# PyTorch Geometric provides us a set of common graph layers\n",
    "# !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.datasets import Planetoid, TUDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289c13f",
   "metadata": {},
   "source": [
    "### Exercise: node-level task <a class=\"anchor\" id=\"gnn-node\"></a>\n",
    "\n",
    "For node-level tasks, GNNs are used to classify nodes in a graph, in a semi-supervised manner: we have a graph with nodes of which only a certain amount of nodes are labeled. The aim of the GNN is to learn those labeled examples during training and try to generalize to the unlabeled nodes.\n",
    "\n",
    "In this guided exercise we will use the [Cora dataset](https://en.wikipedia.org/wiki/CORA_dataset), a citation network consisting of 2708 scientific publications (nodes) with edges between each other, representing the citation of one paper by another. The papers in the graphs are labeled with 7 different labels: Neural Networks, Rule Learning, Reinforcement Learning, Probabilistic Methods, Theory, Genetic Algorithms, Case-Based Reasoning. As the MNIST dataset is widely used as benchmark for computer vision models, this dataset is the equivalent to evaluate the performance of graph neural networks and other graph-based algorithms.\n",
    "\n",
    "Let's start by exploring the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50a689",
   "metadata": {},
   "source": [
    "**Cora dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the cora dataset + metatada\n",
    "# this will download the dataset in your folder\n",
    "cora_dataset = Planetoid(root='./', name=\"Cora\")\n",
    "\n",
    "# get the actual data\n",
    "cora = cora_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore dataset characteristics\n",
    "print('Number of nodes:', cora.num_nodes)\n",
    "print('Number of edges:', cora.num_edges)\n",
    "print('Number of features:', cora_dataset.num_features)\n",
    "print('Number of classes:', cora_dataset.num_classes)\n",
    "print('Has isolated nodes:', cora.has_isolated_nodes())  \n",
    "print('Has self-loops:', cora.has_self_loops())  \n",
    "print('Is undirected:', cora.is_undirected())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f118969",
   "metadata": {},
   "source": [
    "The dataset contains 2708 pubblication and 10556 links. Each node/publication in the dataset is described by a 0/1-valued vector indicating whether a specific word from the dictionary is included or not in the pubblication. The dictionary consists of 1433 unique words (features).\n",
    "\n",
    "We can use NetworkX to plot the graph (the code below will take few minutes to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fed5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = to_networkx(cora, to_undirected=True)\n",
    "\n",
    "# use spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# compute degree centrality\n",
    "# degree represents the number of edges from each node,\n",
    "# the centrality allows us to undertsand the more 'popular' nodes\n",
    "cent = nx.degree_centrality(G)\n",
    "cent_array = np.array(list(cent.values()))\n",
    "\n",
    "# size of nodes will be proportional to their popularity\n",
    "node_size = list(map(lambda x: x * 500, cent.values()))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "#draw nodes\n",
    "nodes = nx.draw_networkx_nodes(G, pos, node_size=node_size,\n",
    "                               cmap=plt.cm.plasma,\n",
    "                               nodelist=list(cent.keys()))\n",
    "# draw edges\n",
    "edges = nx.draw_networkx_edges(G, pos, width=0.25, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15638faa",
   "metadata": {},
   "source": [
    "The papers in the Cora dataset are divided into 7 classes. Let's see which of the classes is more popular (and whether we have an unbalanced dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate labels to class numbers\n",
    "# see https://keras.io/examples/graph/gnn_citations/\n",
    "label_dict = {\n",
    "    0: \"Theory\",\n",
    "    1: \"Reinforcement_Learning\",\n",
    "    2: \"Genetic_Algorithms\",\n",
    "    3: \"Neural_Networks\",\n",
    "    4: \"Probabilistic_Methods\",\n",
    "    5: \"Case_Based\",\n",
    "    6: \"Rule_Learning\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3716d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels\n",
    "labels = cora.y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count instances for each label\n",
    "count_label = [list(labels).count(key) for key in label_dict.keys()]\n",
    "dict(zip(label_dict.values(), count_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd2462",
   "metadata": {},
   "source": [
    "**Task 1 (optional)**\n",
    "\n",
    "Colour-code the nodes in the above graph by subject area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35203c",
   "metadata": {},
   "source": [
    "**Train the model**\n",
    "\n",
    "It's time to train our model. The cora dataset contains 3 objects:  `train_mask`, `val_mask`, and `test_mask`, representing   which nodes we should use for training, validation, and testing, respectively. The `x` tensor is the **feature** tensor of our 2708 publications, and `y` the **labels** for all nodes (see above, already used to count the labels). \n",
    "\n",
    "Note that x and y are [Pytorch tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html),  multi-dimensional arrays that allow for efficient computation within the Pytorch framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17477032",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training nodes:', cora.train_mask.sum().numpy())\n",
    "print('Number of validation nodes:', cora.val_mask.sum().numpy())\n",
    "print('Number of test nodes:', cora.test_mask.sum().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432adf29",
   "metadata": {},
   "source": [
    "**Note**: We have 1640 of labelled data that we are going to use for the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76634890",
   "metadata": {},
   "source": [
    "We are going to use a GCN model structure for our task. The model will contain two GCNConv layers, relu activation, a dropout rate of 0.1, and  16 hidden channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0db44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = cora.num_features\n",
    "dim_out = cora_dataset.num_classes\n",
    "hidden_channels = 16\n",
    "\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # define model components\n",
    "        super(GCNModel, self).__init__()\n",
    "        \n",
    "        # aactivation functions\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # layers\n",
    "        self.conv1 = GCNConv(dim_in, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dim_out)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "    # construct the feed-forward network\n",
    "    def forward(self, data):\n",
    "        # for graph layers, we need the feature of the nodes (the tensor 'x')\n",
    "        # and we need also the \"edge_index\" tensor as additional input\n",
    "        # this represents which node is connected to which other node\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        # layer 1: activate input conv layer (143,16) using ReLu function\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        \n",
    "        # add dropout with probability 0.1\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # layer 2 : activate output conv layer (16,7) using ReLu function\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d57132",
   "metadata": {},
   "source": [
    "The code above can be implemented in a more complex way using Tensorflow: https://keras.io/examples/graph/gnn_citations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ddbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model = GCNModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03edca33",
   "metadata": {},
   "source": [
    "Before start training our model, we want to visualise the node representations  (embeddings) of our untrained GCN network, in other words what the network knows (doesn't know?) at the moment. For this visualisation, we make use of [**TSNE**](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), this is a statistical method to visualise high-dimensional representations onto a 2D plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# the function below takes the nodes embeddings as 'known' by the GCN and the related labels\n",
    "# the function returns a 2D-plot\n",
    "def visualise_embeddings(nodes_embed, labels):\n",
    "    tsne = TSNE(n_components=2).fit_transform(nodes_embed.detach().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1], s=70, c=labels, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GCN knowledge of the nodes embeddings\n",
    "kb = model(cora)\n",
    "\n",
    "# plot the embeddings on a 2D-plan, colour by labels' classes\n",
    "visualise_embeddings(kb, labels=cora.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618176d3",
   "metadata": {},
   "source": [
    "Right now, our GCN knowledge is very messy! Let's train the model and make the network learn about the Cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for the training phase\n",
    "#  compute loss and perform back-propagatio    \n",
    "def train():\n",
    "    model.train()\n",
    "    # set the gradients to zero before starting the backpropagation of the training process\n",
    "    # see this thread for more details: \n",
    "    # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch/48009142#48009142\n",
    "    optimizer.zero_grad()\n",
    "    out = model(cora)\n",
    "    loss = criterion(out[cora.train_mask], cora.y[cora.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "# Helper function for the testing phase\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(cora)\n",
    "    pred = out.argmax(dim=1)\n",
    "    # count correct predictions\n",
    "    acc = (pred[cora.test_mask] == cora.y[cora.test_mask]).sum().item() / cora.test_mask.sum().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cf64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "EPOCHS = 201\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    if epoch % 10 == 0:    # print every 10 epochs\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Test Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d8e1e",
   "metadata": {},
   "source": [
    "Not too bad for being a very simple GCN! Let's what the model knows now about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GCN knowledge of the nodes embeddings\n",
    "kb = model(cora)\n",
    "\n",
    "# plot the embeddings on a 2D-plan, colour by labels' classes\n",
    "visualise_embeddings(kb, labels=cora.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebb322",
   "metadata": {},
   "source": [
    "The 'knowledge' of our system is now more structured!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e5b8c",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "The [**Zachary's karate club network**](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) is a  graph describing the social network of 34 members (nodes) of a karate club,  and members interactions outside the club(edges). There are 4 classes, which represent the community each node/member belongs to.\n",
    "\n",
    "Implement a node-level task GCN to classify memmbers communities into 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffecdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here your code\n",
    "\n",
    "# uncomment the following code to get the dataset\n",
    "#from torch_geometric.datasets import KarateClub\n",
    "# dataset = KarateClub().data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b169d12",
   "metadata": {},
   "source": [
    "### Exercise: edge-level task <a class=\"anchor\" id=\"gnn-edge\"></a>\n",
    "\n",
    "For edge-level tasks, GNNs are used to predict whether there will be/should be an edge between two nodes. This is particualr useful for social network analysis.\n",
    "\n",
    "There are different ideas for doing edge-prediction. Most solutions are based on an encoding-decoding process: we encode our edge information by training a GCN (as done above), we decode the output  by computing a similarity score on the pair of node features. This will be close to 1 if there should be a link, and 0 otherwise.\n",
    "\n",
    "**Task 3**\n",
    "\n",
    "\n",
    "For this task, please review the code in the following [PyTorch link prediction](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/link_pred.py) example. The guided example still uses the Cora dataset,  but this time to predict edges between node points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566abcf7",
   "metadata": {},
   "source": [
    "### Exercise: graph-level task <a class=\"anchor\" id=\"gnn-graph\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ece63",
   "metadata": {},
   "source": [
    "Graph-level tasks refer to the problem of classifiying entire graphs, based on some structural graph properties. For these tasks, the input is a dataset of graphs. These solutions are often applied in the field of chemistry, to classify chemical compounds, these molecules can be represented as graphs, where atoms are nodes and chemical bonds are edges.\n",
    "\n",
    "For our guided exercise we will use the [MUTAG](https://paperswithcode.com/dataset/mutag) dataset.This dataset consists of 188 chemical compounds, each represented as a graph. Each graphs/molecule has around 18 nodes and 20 edges, and is labeled as either mutagenic (positive class) or non-mutagenic (negative class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutag = TUDataset(root='./', name='MUTAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore dataset characteristics\n",
    "print('Number of graphs:', len(mutag))\n",
    "print('Number of features:', mutag.num_features)\n",
    "print('Number of classes:', mutag.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef21a31",
   "metadata": {},
   "source": [
    "Each node in the graph has 7 features (atom types). Let's get some details on a specific graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first graph\n",
    "g = mutag[0]  \n",
    "\n",
    "print('Number of nodes:',g.num_nodes)\n",
    "print('Number of edges:',g.num_edges)\n",
    "print('Graph label:', g.y.numpy().item())\n",
    "print('Has isolated nodes:', g.has_isolated_nodes())\n",
    "print('Has self-loops:',g.has_self_loops())\n",
    "print('Is undirected:',g.is_undirected())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286ef35",
   "metadata": {},
   "source": [
    "We can represent the graph using NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = to_networkx(g, to_undirected=True)\n",
    "\n",
    "nx.draw(G)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17375bf2",
   "metadata": {},
   "source": [
    "We can start training our model. After shuffling the graphs in our dataset, we split them into train (80%) and testing (20%) by considering the first 150 graph for the training and the remaining ones for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ef1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set randm seed for the shuffling\n",
    "torch.manual_seed(42)\n",
    "# shuffle dataset\n",
    "mutag = mutag.shuffle()\n",
    "\n",
    "# create train and test split\n",
    "train_dataset = mutag[:150]\n",
    "test_dataset = mutag[150:]\n",
    "\n",
    "print('Number of training graphs:',len(train_dataset))\n",
    "print('Number of test graphs:',len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd7f10",
   "metadata": {},
   "source": [
    "While training a model, we  want to pass data in 'batches', reshuffled data at every epoch to reduce model overfitting. In PyTorch this is done using [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders), an iterable pytorch construct that allows to create batches to feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac780c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5ce11",
   "metadata": {},
   "source": [
    "For the structure of our model, we can reuse the same structure used for the node-level task. However, since in a graph-level task the output is a label, and not the entire graph structure as in the node classification task, we need to add an average pool and a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bec0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "dim_in = mutag.num_node_features\n",
    "dim_out = mutag.num_classes\n",
    "hidden_channels = 16\n",
    "\n",
    "class GCNModelGraph(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCNModelGraph, self).__init__()\n",
    "        \n",
    "        # aactivation functions\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        # layers\n",
    "        self.conv1 = GCNConv(dim_in, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dim_out)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "        \n",
    "    def forward(self, data):\n",
    "        # for graph layers, we need the feature of the nodes (the tensor 'x')\n",
    "        # and we need also the \"edge_index\" tensor as additional input\n",
    "        # this represents which node is connected to which other node\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # layer 1: activate input conv layer (7,16) using ReLu function\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        \n",
    "        # add dropout with probability 0.1\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # layer 2 : activate  conv layer (16,16) using ReLu function\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "\n",
    "        # average pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # get final classifier\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCNModelGraph()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model = GCNModelGraph()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these helper functions are similar to the one used above\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    # iterate through the batches generated by the Dataloader\n",
    "    for data in train_loader:\n",
    "        # set the gradients to zero before starting the backpropagation of the training process\n",
    "        # see this thread for more details: \n",
    "        # https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch/48009142#48009142\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct_pred = 0\n",
    "    # iterate through the batches generated by the Dataloader\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        # count correct predictions\n",
    "        correct_pred += int((pred == data.y).sum())\n",
    "    return correct_pred / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "EPOCHS = 201\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    if epoch % 10 == 0:    # print every 10 epochs\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Test Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b460fea",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "\n",
    "The accuracy obtained is not very high. What happens by adding a 3rd GCNConv and increasing the dropout probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bdde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646dd1e7",
   "metadata": {},
   "source": [
    "**Task 5**\n",
    "\n",
    "The MUTAG dataset is part of a large collection of different graph classification datasets, known as the [TUDatasets](https://chrsmrrs.github.io/datasets/). All these datasets are directly accessible through PyTorch  Geometric [`torch_geometric.datasets.TUDataset`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset).\n",
    "\n",
    "Implement a similar GCN system with another TUDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8863eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write here your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
